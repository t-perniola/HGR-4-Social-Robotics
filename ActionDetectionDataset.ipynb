{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importiamo il modello Holistic di MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "funzione di riconoscimento: convertiamo prima l'immagine dal canale BGR (output di OpenCV feed) a quello RGB, necessario per MediaPipe;\n",
    "alla fine riconvertiamo in BGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction with the Holistic model\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ora visualizziamo i landmark graficamente, in maniera standard; li renderizziamo -> saranno visibili a schermo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "#le X_CONNECTIONS sono mappe che restituiscono i legami fra i vari landmark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "formattiamo, personalizziamo i landmark in maniera grafica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=1, circle_radius=1)\n",
    "                             ) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restituisce il numero dei landmark usati per la parte del corpo specificata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lavoriamo su ciascun landkmark: \n",
    "\n",
    "- creiamo degli array dove memorizzare le sue coordinate (dei landmark)\n",
    "\n",
    "- Useremo degli array di zero quando una certa parte del corpo non è riconosciuta/presente, per non fare andare in errore il sistema."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facciamo la stessa cosa, ma tutto su una sola linea.\n",
    "\n",
    "**N.B.** usiamo *flatten()* perchè vogliamo un solo array, unidimensionale, perchè ci servirà questo formato per la LSTM; infatti, senza flatten() avremo una matrice, un array bidimensionale (ciascun landmark avrà *x* valori associati)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "come abbiamo detto prima, creiamo un array vuoto (di zeri), per gestire l'errore nel caso in cui una parte del corpo non è riconosciuta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qui facciamo quello fatto sopra, ma in maniera pù organizzata, in una funzione apposita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nello specifico:\n",
    "\n",
    "res.x, res.y, res.z sono i singoli valori delle coord. per ciascun landmark, che verranno inseriti in un array più grande, ciclando per ogni landmark:\n",
    "\n",
    "- se non esistono coordinate per i landmark, quindi quella parte del corpo non è riconosciuta, viene creato un array vuoto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abbiamo estratto i **keypoints** per il frame di interesse, che saranno utili per il riconoscimento dei gesti/segni"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Folders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quali sono i gesti contenuti nel dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_file = pd.read_csv(r'D:\\TMS\\dataset\\jester\\labels\\labels.csv') #TODO rel path\n",
    "\n",
    "labels = labels_file['labels'].tolist() #metto in una lista tutte le labels del dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memorizzo la lista di tutti i video disponibili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = r'D:\\TMS\\dataset\\jester\\20bn-jester-v1'\n",
    "\n",
    "list_video = sorted(os.listdir(DATADIR), key=len) #li voglio ordinati così come compaiono nell'explorer dei file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creo fold in cui salvare keypoints, per ogni video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid in list_video: #per ogni video\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATADIR, str(vid), \"keypoints\"))\n",
    "        except:\n",
    "            pass #se esistono già, skippa la creazione"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creo ora folds dei gesti, dove salvaremo ogni video in cui la stessa è performata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels: #per ogni label (per ogni gesto)\n",
    "    try:\n",
    "        os.makedirs(os.path.join(DATADIR, label))\n",
    "    except:\n",
    "        pass #se esistono già, skippa la creazione"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ora dovremmo spostare in queste cartelle i relativi video"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Collect Keypoint Values from the dataset for Training and Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗❗ Estrarremo i keypoints dai frame di ciascun video presente nel **dataset** *Jester*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struttura del dataset Jester:\n",
    "\n",
    "- **VIDEO:** 148092\n",
    "    - **FRAME x video:** circa 30\n",
    "        - *FPS:* 12, un frame ogni 12 s\n",
    "        - *altezza:* 100px\n",
    "\n",
    "Il dataset è inoltre già splittato negli insiemi di **Training, Test e Validation:**\n",
    "\n",
    "- **TRAINING videos:** 118562 \n",
    "- **TEST videos:** 14743 \n",
    "- **VALIDATION videos:** 14787 \n",
    "\n",
    "Il dataset conterrà **27** labels (classi) di gesti diversi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "otteniamo i video per il **training** del modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34870</td>\n",
       "      <td>Drumming Fingers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56557</td>\n",
       "      <td>Sliding Two Fingers Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129112</td>\n",
       "      <td>Sliding Two Fingers Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63861</td>\n",
       "      <td>Pulling Two Fingers In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131717</td>\n",
       "      <td>Sliding Two Fingers Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118557</th>\n",
       "      <td>75507</td>\n",
       "      <td>Swiping Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118558</th>\n",
       "      <td>48433</td>\n",
       "      <td>Sliding Two Fingers Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118559</th>\n",
       "      <td>146421</td>\n",
       "      <td>Sliding Two Fingers Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118560</th>\n",
       "      <td>49514</td>\n",
       "      <td>Thumb Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118561</th>\n",
       "      <td>4502</td>\n",
       "      <td>Sliding Two Fingers Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118562 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           key                      value\n",
       "0        34870           Drumming Fingers\n",
       "1        56557  Sliding Two Fingers Right\n",
       "2       129112   Sliding Two Fingers Down\n",
       "3        63861     Pulling Two Fingers In\n",
       "4       131717     Sliding Two Fingers Up\n",
       "...        ...                        ...\n",
       "118557   75507               Swiping Down\n",
       "118558   48433   Sliding Two Fingers Left\n",
       "118559  146421  Sliding Two Fingers Right\n",
       "118560   49514                   Thumb Up\n",
       "118561    4502     Sliding Two Fingers Up\n",
       "\n",
       "[118562 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = pd.read_csv(r'D:\\TMS\\dataset\\jester\\labels\\train.csv', delimiter=\";\") #TODO path relativo\n",
    "\n",
    "train_list = train_file['key'].tolist() #lista degli indici dei video attualmente estratti\n",
    "train_file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "otteniamo ora i video per il **testing** del modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115190</td>\n",
       "      <td>Turning Hand Counterclockwise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49340</td>\n",
       "      <td>Turning Hand Clockwise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118693</td>\n",
       "      <td>Shaking Hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141855</td>\n",
       "      <td>Pulling Hand In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138995</td>\n",
       "      <td>Thumb Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14738</th>\n",
       "      <td>82713</td>\n",
       "      <td>Swiping Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14739</th>\n",
       "      <td>129898</td>\n",
       "      <td>Zooming Out With Two Fingers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14740</th>\n",
       "      <td>86265</td>\n",
       "      <td>Swiping Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14741</th>\n",
       "      <td>140844</td>\n",
       "      <td>Zooming Out With Full Hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14742</th>\n",
       "      <td>58593</td>\n",
       "      <td>Shaking Hand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14743 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          key                          value\n",
       "0      115190  Turning Hand Counterclockwise\n",
       "1       49340         Turning Hand Clockwise\n",
       "2      118693                   Shaking Hand\n",
       "3      141855                Pulling Hand In\n",
       "4      138995                     Thumb Down\n",
       "...       ...                            ...\n",
       "14738   82713                     Swiping Up\n",
       "14739  129898   Zooming Out With Two Fingers\n",
       "14740   86265                  Swiping Right\n",
       "14741  140844     Zooming Out With Full Hand\n",
       "14742   58593                   Shaking Hand\n",
       "\n",
       "[14743 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = pd.read_csv(r'D:\\TMS\\dataset\\jester\\labels\\test-answers.csv', delimiter=\";\") #TODO path relativo\n",
    "\n",
    "test_list = test_file['key'].tolist() #lista degli indici dei video attualmente estratti\n",
    "test_file = test_file.loc[:, ~test_file.columns.str.contains('^Unnamed')]  #cancello colonne non necessarie\n",
    "test_file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **N.B.** Vediamo ora tre modi diversi per estrarre i frame, a seconda della *fonte* (camera, dataset, ...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ##### **qui** ⬇️ leggiamo semplicemente i frame dal dataset, *già* estratti dai video"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*prima però bisogna considerare che non ho estratto dall'archivio .tar tutti i video ...*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vediamo prima il **training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "my_train_list = [] #lista che contiene i miei video di training, attualmente disponibili\n",
    "\n",
    "#mostra tutti gli indici dei video attualmente estratti dall'archivio tar, convertiti in integers\n",
    "list_video_int = list(map(int, list_video))\n",
    "\n",
    "for vid in list_video_int:\n",
    "    for train_vid in train_list:\n",
    "        if vid == train_vid:\n",
    "            count += 1     \n",
    "            my_train_list.append(vid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attualmente, abbiamo **30228** video di training su 37793 totali (80% circa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"mytrainlist\", my_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30228"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_train_list = np.load(\"mytrainlist.npy\").tolist()\n",
    "len(my_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_file = train_file.query('key in @my_train_list') #sottoinsieme di train_file, che contiene solo i video di training attualm. disponibili\n",
    "train_list2 = sorted(my_train_file['key'].tolist())  #lista degli indici dei video di training attualmente disponibili"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "controllo effettivamente se le liste sono uguali: in tal caso tengo conto solamente di una delle due"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lists are equal\n"
     ]
    }
   ],
   "source": [
    "if(set(my_train_list) == set(train_list2)):\n",
    "    print(\"Lists are equal\")\n",
    "else:\n",
    "    print(\"Lists are not equal\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- facciamo lo stesso per il **test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "my_test_list = [] #lista che contiene i miei video di test, attualmente disponibili\n",
    "\n",
    "#mostra tutti gli indici dei video attualmente estratti dall'archivio tar, convertiti in integers\n",
    "list_video_int = list(map(int, list_video))\n",
    "\n",
    "for vid in list_video_int:\n",
    "    for test_vid in test_list:\n",
    "        if vid == test_vid:\n",
    "            count += 1     \n",
    "            my_test_list.append(vid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attualmente abbiamo **3763** video di test su 37793 totali (10% circa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"mytestlist\", my_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3763"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test_list = np.load(\"mytestlist.npy\").tolist()\n",
    "len(my_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_file = test_file.query('key in @my_test_list') #sottoinsieme di test_file, che contiene solo i video di testing attualm. disponibili\n",
    "test_list2 = sorted(my_test_file['key'].tolist())  #lista degli indici dei video di testing attualmente disponibili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lists are equal\n"
     ]
    }
   ],
   "source": [
    "if(set(my_test_list) == set(test_list2)):\n",
    "    print(\"Lists are equal\")\n",
    "else:\n",
    "    print(\"Lists are not equal\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ⬜️ devo ottenere tutte le label associate alla lista di indici/key ottenuta precedentemente\n",
    "- ✅ estrarre i keypoints da ciascun frame di ogni video (solo 5k per ora)\n",
    "    - controllare che siano tutti\n",
    "- ✅ dove salvare poi i keypoints? creare fold all'interno della fold di ogni video in cui memorizzare i keypoints in forma di array\n",
    "- ⬜️ resta poi da organizzare i video in base alle azioni eseguite al loro interno"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accedo ai frame di ogni video e ne estraggo i keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = r'D:\\TMS\\dataset\\jester\\20bn-jester-v1'\n",
    "\n",
    "actual_video = []\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "\n",
    "    for vid in my_train_list:  # itero su tutti i video ...\n",
    "\n",
    "        # create path (...\\20bn-jester-v1\\000xx)\n",
    "        path = os.path.join(DATADIR, str(vid))\n",
    "        print(\"\\nPercorso video:\", path)\n",
    "\n",
    "        # restituisce tutti gli elementi nel path indicato, nel nostro caso tutti i frame per il video specificato con il suo path\n",
    "        frames_4_video = os.listdir(path)\n",
    "        \n",
    "        num_frames = len(frames_4_video) - 1 #togliamo 1 perchè non consideriamo la fold dei keypoints\n",
    "        print(\"Numero frame per questo video:\", num_frames)\n",
    "\n",
    "        all_frames = []\n",
    "\n",
    "        keypoints_path = os.path.join(path, 'keypoints')\n",
    "\n",
    "        num_keypoints = len(os.listdir(keypoints_path)) #indica quanti keypoints sono stati estratti            \n",
    "        \n",
    "        # esegui se la fold dei keypoints è vuota ed i keypoints non sono ancora stati estratti da ogni frame \n",
    "        if not os.listdir(keypoints_path) and num_keypoints != num_frames:\n",
    "            \n",
    "            # iterate over each image (00001, 00002, ...)S\n",
    "            for img in frames_4_video:            \n",
    "                        \n",
    "                if not img == 'keypoints': #non considero la cartella dove salvo i keypoints (TODO potrei farlo prima, escludendo la cartella)\n",
    "                    \n",
    "                    img_num = img[0:5] #prendo i primi N char, per non avere l'estensione nel nome quando salvo dopo\n",
    "\n",
    "                    img_path = os.path.join(path, img)\n",
    "                    img_array = cv2.imread(img_path)  # convert to array\n",
    "                    print(img_path)                    \n",
    "                    all_frames.append(img)\n",
    "\n",
    "                    # estraggo e memorizzo i keypoints\n",
    "                    image, results = mediapipe_detection(img_array, holistic)\n",
    "                    draw_styled_landmarks(image, results)\n",
    "                    print(results)\n",
    "                    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))                    \n",
    "                    keypoints = extract_keypoints(results)\n",
    "\n",
    "                    # salvo i keypoints su disco\n",
    "                    npy_path = os.path.join(DATADIR, str(vid), \"keypoints\", img_num)\n",
    "                    np.save(npy_path, keypoints)\n",
    "            \n",
    "        else:\n",
    "            print(\"N.B. keypoints già estratti per questo video\")\n",
    "            actual_video.append(vid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opero altra suddivisione (attualmente 5k circa video disponibili) per **TRAINING SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59351</th>\n",
       "      <td>4</td>\n",
       "      <td>Swiping Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82397</th>\n",
       "      <td>6</td>\n",
       "      <td>Drumming Fingers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106437</th>\n",
       "      <td>10</td>\n",
       "      <td>Thumb Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69146</th>\n",
       "      <td>12</td>\n",
       "      <td>Sliding Two Fingers Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69109</th>\n",
       "      <td>20</td>\n",
       "      <td>Doing other things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46264</th>\n",
       "      <td>5412</td>\n",
       "      <td>Sliding Two Fingers Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118306</th>\n",
       "      <td>5416</td>\n",
       "      <td>Sliding Two Fingers Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62180</th>\n",
       "      <td>5423</td>\n",
       "      <td>Pulling Hand In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108093</th>\n",
       "      <td>5428</td>\n",
       "      <td>Swiping Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74809</th>\n",
       "      <td>5430</td>\n",
       "      <td>Pushing Hand Away</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1110 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         key                     value\n",
       "59351      4             Swiping Right\n",
       "82397      6          Drumming Fingers\n",
       "106437    10                Thumb Down\n",
       "69146     12  Sliding Two Fingers Down\n",
       "69109     20        Doing other things\n",
       "...      ...                       ...\n",
       "46264   5412  Sliding Two Fingers Left\n",
       "118306  5416    Sliding Two Fingers Up\n",
       "62180   5423           Pulling Hand In\n",
       "108093  5428             Swiping Right\n",
       "74809   5430         Pushing Hand Away\n",
       "\n",
       "[1110 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_train_file = my_train_file.query('key in @actual_video') #contiene indice e label dell'azione associata a quel video\n",
    "\n",
    "my_train_file.sort_values([\"key\"], axis=0, ascending=[True], inplace=True) #ordino gli elem in modo crescente in base al valore dell'idx del video \n",
    "my_train_file #questo è il file su cui devo lavorare come training set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "controllo le coordinate dei keypoints estratti per un frame a caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51776236,  0.4528217 , -0.93003947, ...,  0.35263199,\n",
       "        0.48780304, -0.07810371])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(r\"D:\\TMS\\dataset\\jester\\20bn-jester-v1\\5430\\keypoints\\00015.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TEST 2:** prendo una precisa immagine dal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = r'D:\\TMS\\dataset\\jester\\20bn-jester-v1\\5454'\n",
    "\n",
    "# OTTENGO I FRAME    \n",
    "path = os.path.join(DATADIR, '00003.jpg')  # create path (...\\Color\\rgb1)\n",
    "frame = cv2.imread(path)  # convert to array\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "provo con modello *Holistic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTRAGGO I KEYPOINTS DA CIASCUN FRAME\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "    image, results = mediapipe_detection(frame, holistic)\n",
    "    draw_styled_landmarks(image, results)\n",
    "    print(results)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  \n",
    "\n",
    "    keypoints = extract_keypoints(results) #memorizzo i keypoints estratti     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "provo solamente con modello *Hands*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTRAGGO I KEYPOINTS DA CIASCUN FRAME\n",
    "with mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5) as hands:\n",
    "    image, results = mediapipe_detection(frame, hands)\n",
    "    \n",
    "    annotated_image = image.copy()\n",
    "    \n",
    "    for hand_landmarks in results.multi_hand_landmarks:        \n",
    "\n",
    "        mp_drawing.draw_landmarks(     \n",
    "            annotated_image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(245,117,66), thickness=1, circle_radius=1),\n",
    "            mp_drawing.DrawingSpec(color=(245,66,230), thickness=1, circle_radius=1)\n",
    "            )\n",
    "\n",
    "    print(results)\n",
    "    plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ##### **qui** ⬇️ estraiamo i frame direttamente dai *video*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFrameNumber = 50\n",
    "cap = cv2.VideoCapture(\"video.mp4\")\n",
    "\n",
    "# get total number of frames\n",
    "totalFrames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "# check for valid frame number\n",
    "if myFrameNumber >= 0 & myFrameNumber <= totalFrames:\n",
    "    # set frame position\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES,myFrameNumber)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ##### **qui** ⬇️ estraiamo i frame direttamente dalla *videocamera*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creiamo un dizionario dove salviamo tutte le azioni/gesti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)} #iteriamo su ciascuna azione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strutturiamo i dati fin ora raccolti, inserendoli in un unico array\n",
    "\n",
    "- **sequences** avrà 90 video al suo interno (30 x 3 azioni), ciascuno di 30 frame, ognuno con 1662 keypoints\n",
    "    - quindi sarà un array *tridimensionale*: (90, 30, 1662)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], [] # rispettivamente X (feature input) e Y (feature target)\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = [] #rappresenta un unico video: insieme di 30 frame\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action]) #ci saranno 90 video al suo interno,\n",
    "        # array monodimensionale (ci dice per ogni video quale azione è presente al suo interno: la prima, ... -> definizione di LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trasformo feature categoriche in feature numeriche (One Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splittiamo i dati in Training Sets e Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addestriamo la rete neurale (ricorrente) **LSTM**: \n",
    "\n",
    "- perchè usare MediaPipe Holistic + una rete neurale LSTM?\n",
    "    - occorrono *meno dati*\n",
    "    - più *veloce* nell'addestramento\n",
    "    - riconoscimento più *veloce* <- RN più *semplice*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorBoard** ci offre una web app per monitorare il nostro modello: addestramento, accuratezza, ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il modello Sequential ci permette di aggiungere layer in mainera organizzata e facile\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662))) #64 unità\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu')) #return_sequences è neecessario per il passaggio di dati al layer successivo\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax')) #distribuzione di probabilità che assegna ad ogni azione una certa prob."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiliamo il modello, usando:\n",
    "\n",
    "- loss function: \"categorical cross entropy\"\n",
    "    - è obbligatoria per classificazione multi-classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=500, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test) #prendo tutte le predizioni dall'insieme di test\n",
    "# res[0]: significa che prendo la prima prediction (quindi la prediction della prima azione presente in X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "poichè ad ogni azione è associata una probabilità, la mia predizione sarà quella che avrà probabilità **maggiore** fra esse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[4])] #qual'è l'azione con il più altro valore di probabilità associato?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prendo la label della 4a azione: vedo quindi il *ground truth*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ovviamente, se le due azioni **coincidono** significa che il modello ha una **buona accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, actions, image, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6976a0d29688b69af430d154ca21ca8f69eff21986dd9c09ddc9cd50d921dcd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
